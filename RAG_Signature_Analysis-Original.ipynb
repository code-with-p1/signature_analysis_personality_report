{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40c5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22af8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain \\\n",
    "#     langchain-community \\\n",
    "#     langchain-huggingface \\\n",
    "#     faiss-cpu \\\n",
    "#     pypdf \\\n",
    "#     sentence-transformers \\\n",
    "#     huggingface_hub \\\n",
    "#     langchain-google-genai \\\n",
    "#     tf-keras \\\n",
    "#     hf_xet \\\n",
    "#     torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6ca977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From g:\\Project - Automated Signature Analysis and Personality Report\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e64adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Models...\")\n",
    "\n",
    "chat_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.65,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=chat_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ff2765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs...\n",
      "→ Loaded 5 pdf pages\n",
      "→ Created 10 chunks\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading PDFs...\")\n",
    "\n",
    "PDF_FOLDER = \"./RAG_Documents\"\n",
    "CHUNK_SIZE = 850\n",
    "CHUNK_OVERLAP = 120\n",
    "\n",
    "loader = PyPDFDirectoryLoader(PDF_FOLDER)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"→ Loaded {len(docs)} pdf pages\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"→ Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72249ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings... (this may take a few minutes)\n",
      "→ FAISS index created!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings... (this may take a few minutes)\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "print(\"→ FAISS index created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16351e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Conscientiousness\"\n",
    "\n",
    "system_message = \"\"\"You are talented graphalogy expert who has PHD in it. You are best in reading and analysing the handwritting. You are a helpful assistant. Answer ONLY from the provided transcript context. If the context is insufficient, just say you don't know.\"\"\"\n",
    "\n",
    "question = \"Explain the detailed analysis on shared topic\"\n",
    "\n",
    "docs = retriever.invoke(f\"Explain Summary, Writing-style descriptions, Graphology-style Overall impression on given topic : {topic}\")\n",
    "context = \"\\n\\n\".join(f\"[Document {i+1}]\\n{doc.page_content}\\n\" for i, doc in enumerate(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297dc552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "------------------------------------------------------------\n",
      "The detailed analysis for Conscientiousness is as follows:\n",
      "\n",
      "1. **Writing Style Descriptions**: The writing is structured, clear, and detail-oriented. Sentences are well-organized, grammatically careful, and goal-focused. This indicates a strong emphasis on planning, responsibility, accuracy, and logical progression of ideas.\n",
      "\n",
      "2. **Gaphology-Style Overall Impression**: The writing is controlled, disciplined, and task-focused. This suggests a person who is highly organized, dependable, and goal-oriented.\n",
      "\n",
      "3. **Slant**: The slant is vertical or slightly right. This indicates emotional control, objectivity, and self-regulation, traits that are consistent with high conscientiousness.\n",
      "\n",
      "4. **Pressure**: The pressure is firm and consistent, which indicates determination, reliability, and a strong sense of duty. This reflects a person who is committed and persistent in their tasks.\n",
      "\n",
      "5. **Spacing**: The spacing is tight but regular, suggesting efficiency, focus, and careful planning. This indicates that the writer values precision and thoroughness in their work.\n",
      "\n",
      "Overall, the detailed analysis of handwriting related to conscientiousness shows a person who is highly organized, dependable, and goal-oriented. Their writing is structured, clear, and detail-oriented, with a strong emphasis on planning, responsibility, and accuracy. The controlled and disciplined nature of their writing, along with their firm and consistent pressure and tight but regular spacing, further reinforces this trait.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_message}\"),\n",
    "    (\"human\", \"\"\"Context information:\\n\\n{context}\\n\\nQuestion:\\n\\n{question}\\n\\nTopic:{topic}\\n\\nAnswer:\"\"\")\n",
    "])\n",
    "\n",
    "simple_rag_chain = (\n",
    "    rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Just invoke with dictionary of the three variables\n",
    "answer = simple_rag_chain.invoke({\n",
    "    \"system_message\": system_message, # or just pass topic and build inside\n",
    "    \"context\": context,\n",
    "    \"question\": question,\n",
    "    \"topic\": topic\n",
    "})\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(\"-\" * 60)\n",
    "print(answer)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a9e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
